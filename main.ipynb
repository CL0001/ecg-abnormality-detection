{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desease Prediction by ECG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're importing the libraries we need for preprocessing the ECG data:\n",
    "\n",
    "- `os` lets us handle file paths and directories.\n",
    "- `numpy (np)` is for numerical operations and working with arrays.\n",
    "- `wfdb` is a library specialized for reading and processing physiological signals like ECGs from PhysioNet-formatted files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import wfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our ECG preprocessing pipeline is to configure the file paths and load the necessary data resources. We begin by specifying:\n",
    "- the directory containing the raw ECG signals,\n",
    "- the path to the label annotation file (CSV), and\n",
    "- the output directory, where we will later store the preprocessed and batched data.\n",
    "\n",
    "We also define a variable `min_length`, set to 2200, which corresponds to the shortest ECG recording in our dataset. This ensures all signals are trimmed or padded to a uniform length, making them suitable for model training. Additionally, we initialize an empty list named `batch` that will temporarily hold a group of ECG signals before writing them to disk.\n",
    "\n",
    "Once paths and parameters are set, we proceed to load the label annotations from the CSV file. Since the file contains a header, we skip the first row. The labels are initially loaded as strings, after which we remove unnecessary columns — specifically, the first four and the last one — to retain only the useful class information. Finally, we convert the cleaned label values to `float32`, which is the required numerical format for machine learning workflows.\n",
    "\n",
    "We also prepare for batching by defining a base filename (\"`batch`\") and initializing a `file_group` counter. Lastly, we ensure the output directory exists by creating it if necessary using `os.makedirs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./ecg_resources/data\"\n",
    "label_path = \"./ecg_resources/annotations.csv\"\n",
    "output_path = \"./data\"\n",
    "\n",
    "min_length = 2200 # this is length of the shortest timestamp\n",
    "batch = []\n",
    "\n",
    "labels = np.loadtxt(label_path, delimiter=',', skiprows=1, dtype=str)\n",
    "trimed_labels = np.delete(labels, [0, 1, 2, 3, -1], axis=1)\n",
    "casted_labels = trimed_labels.astype(np.float32)\n",
    "\n",
    "filename = \"batch\"\n",
    "file_group = 1\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our environment prepared, we now move on to the core data processing loop. This part of the pipeline iterates through the available ECG records, trims them to a uniform length, batches them, and saves them in `.npz` format along with their corresponding labels.\n",
    "\n",
    "We begin by looping through the data using the `file_group` counter, starting from 1 up to 39,999. For each iteration:\n",
    "- We construct the file name of the ECG record using the expected naming convention: `TNMG{file_group}_N1`.\n",
    "- We then attempt to read the signal using the `wfdb` library, which parses the MIT-BIH compatible record format.\n",
    "If the signal is successfully read, we check its length. To ensure consistency across the dataset:\n",
    "- If the signal is longer than `min_length`, we trim it down to the target length.\n",
    "- If it’s already shorter or equal in length, we keep it as is.\n",
    "\n",
    "Each processed signal is added to the `batch` list. If an exception occurs while loading a file (e.g., the file does not exist or is corrupted), the error is logged, and the loop simply moves to the next file.\n",
    "\n",
    "Once the `batch` reaches 100 signals, we perform the following steps:\n",
    "- Convert the list of signals into a NumPy array with `float32` precision.\n",
    "- Take the first 100 labels from the `casted_labels` array to match the batch.\n",
    "- Save both the signals and labels into an `.npz` file named based on the batch index (e.g., `batch-1.npz`, `batch-2.npz`, etc.).\n",
    "- Clear the `batch` and trim the used labels from `casted_labels` to prepare for the next group.\n",
    "\n",
    "This continues until all records up to 39,999 have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while file_group <= 39_999:\n",
    "    record_name = f\"TNMG{file_group}_N1\"\n",
    "    print(f\"Processing record: {record_name}\")\n",
    "    try:\n",
    "        record_signal = wfdb.rdrecord(os.path.join(data_path, record_name)).p_signal\n",
    "\n",
    "        if record_signal.shape[0] > min_length:\n",
    "            trimmed_signal = record_signal[:min_length, :]\n",
    "        else:\n",
    "            trimmed_signal = record_signal\n",
    "\n",
    "        batch.append(trimmed_signal)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Can't load file: {record_name}, error: {e}\\n\")\n",
    "        file_group += 1\n",
    "        continue\n",
    "\n",
    "    if len(batch) == 100:\n",
    "        numpy_array = np.array(batch, dtype=np.float32)\n",
    "        trimmed_labels = casted_labels[:100]\n",
    "\n",
    "        np.savez(os.path.join(output_path, f\"{filename}-{int(file_group / 100)}.npz\"),\n",
    "                 signals=numpy_array, labels=trimmed_labels)\n",
    "\n",
    "        print(f\"Batch saved as {filename}-{int(file_group / 100)}.npz\\n\")\n",
    "\n",
    "        batch = []\n",
    "        casted_labels = casted_labels[100:]\n",
    "\n",
    "    file_group += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our ECG data has been processed and saved in `.npz` batches, we move on to the next step: preparing a PyTorch-compatible dataset. This will allow us to easily load, shuffle, and feed the data into our neural network model during training.\n",
    "\n",
    "Before we begin, we import PyTorch and verify that it's installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by specifying the directory where the `.npz` files are stored, using `input_path`. This path points to the folder containing the processed signal batches.\n",
    "\n",
    "Next, we initialize two empty lists: `data` and `labels`. These will store all the ECG signal data and corresponding labels, respectively.\n",
    "\n",
    "We then loop through each file in the `input_path` directory. For each file:\n",
    "- We construct the full file path using `os.path.join`.\n",
    "- We load the `.npz` file using `np.load()`, which contains both the signals and labels.\n",
    "- The `signals` and `labels` arrays are extracted from the loaded file and added to the `data` and `labels` lists.\n",
    "\n",
    "Once all the files have been processed, we convert the `data` and `labels` lists into NumPy arrays. This is necessary for efficient handling of the data within PyTorch.\n",
    "\n",
    "Finally, we save the entire dataset (signals and labels) to a single `.pt` file using `torch.save()`. This allows us to quickly reload the dataset during training, ensuring it’s ready for use with PyTorch’s DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./data\"\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i, file in enumerate(os.listdir(input_path)):\n",
    "    print(f\"{i}. processing file: {file}\")\n",
    "    file_path = os.path.join(input_path, file)\n",
    "\n",
    "    record = np.load(file_path)\n",
    "    signals = record[\"signals\"]\n",
    "    label = record[\"labels\"]\n",
    "\n",
    "    data.append(signals)\n",
    "    labels.append(label)\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "torch.save({'data': data, 'labels': labels}, 'dataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset saved in a `.pt` file and ready for use, the next step is to wrap it into a format that PyTorch can work with during training and evaluation. This is done by creating a custom Dataset class that inherits from `torch.utils.data.Dataset`.\n",
    "\n",
    "We begin by importing the required base class from PyTorch’s data utilities.\n",
    "\n",
    "We then define a class called `ECGDataset`, which takes in two arguments during initialization: `data` and `labels`. These are expected to be NumPy arrays (or already-converted tensors) that represent the ECG signal data and their corresponding annotations.\n",
    "\n",
    "Inside the `__init__` method:\n",
    "- The input `data` and `labels` are converted into PyTorch tensors with a float32 data type, ensuring compatibility with typical model architectures.\n",
    "- The signal data is reshaped from its original multi-dimensional structure into a 2D tensor using `.view(-1, 2200 * 8)`. This flattens each ECG signal sample (2200 timestamps with 8 channels) into a single vector, making it suitable as input to fully connected neural networks.\n",
    "- The labels are reshaped into a 2D tensor with shape `(-1, 6)`, where each sample is associated with 6 output values (assumed to be regression targets or class scores).\n",
    "- We also store the total number of samples using `self.data.shape[0]`, which will be returned when PyTorch queries the length of the dataset.\n",
    "\n",
    "The `__len__` method simply returns this number of samples, allowing PyTorch to iterate over the dataset correctly.\n",
    "\n",
    "The `__getitem__` method retrieves a single data-label pair at a given index. This method is essential for PyTorch’s `DataLoader` to be able to batch and shuffle the data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        self.data = self.data.view(-1, 2200 * 8)\n",
    "        self.labels = self.labels.view(-1, 6)\n",
    "\n",
    "        self.num_samples = self.data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data and dataset structure in place, the next step is to define a neural network model suitable for multi-label classification of ECG signals. In this case, we implement a simple feedforward neural network using PyTorch’s `nn.Module` base class.\n",
    "\n",
    "We begin by importing the necessary neural network components from PyTorch.\n",
    "\n",
    "Next, we define a class called `MultiLabelClassifier`, which inherits from `nn.Module`. This model is designed to handle multi-label classification, where each input sample may belong to multiple categories simultaneously.\n",
    "\n",
    "Inside the `__init__` method, we define the architecture of the network:\n",
    "- **Input Layer to Hidden Layer:** The first fully connected layer (`fc1`) transforms the input features (flattened ECG signal vectors) to a specified number of hidden units (`hidden_dim`).\n",
    "- **Activation Function:** A ReLU activation (`self.relu`) is applied to introduce non-linearity, helping the model learn complex patterns in the data.\n",
    "- **Hidden Layer to Output Layer:** The second fully connected layer (`fc2`) maps the hidden representation to the desired output dimension (`output_dim`), which corresponds to the number of labels per sample.\n",
    "- **Output Activation:** A Sigmoid function is used at the end, ensuring each output value lies between 0 and 1. This is suitable for multi-label classification, as each output can be interpreted as the independent probability of a label being present.\n",
    "\n",
    "The `forward` method defines the forward pass of the model—how data flows through the layers during inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before initializing the model or beginning training, we need to determine where the computations will take place—on a GPU (if available) or on the CPU.\n",
    "\n",
    "PyTorch provides a convenient way to check for GPU availability through `torch.cuda.is_available()`. Based on this check:\n",
    "- If a CUDA-compatible GPU is available, we use `cuda` as the device, which enables faster training.\n",
    "- Otherwise, we fall back to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the device selected, we now set up the model architecture, the loss function, and the optimizer that will be used during training.\n",
    "\n",
    "We begin by defining the dimensions:\n",
    "- `input_dim = 2200 * 8`: Each ECG sample has 8 channels (leads), and each is trimmed to 2200 time steps, so we flatten each sample into a vector of size 17,600.\n",
    "- `hidden_dim = 128`: This is the size of the hidden layer in the model. You can tune this value depending on model complexity.\n",
    "- `output_dim = 6`: There are six output classes, each representing a different ECG abnormality label. Since this is a multi-label classification problem, the model predicts six independent probabilities per sample.\n",
    "\n",
    "Next, we initialize the model using our custom `MultiLabelClassifier` class and move it to the appropriate device (CPU or GPU) using `.to(device)`.\n",
    "\n",
    "We define the loss function using **binary cross-entropy** (`BCELoss`), which is suitable for multi-label classification tasks where each label is independent and binary.\n",
    "\n",
    "Finally, we set up the **Adam optimizer**, which adapts the learning rate during training for efficient convergence. We pass in the model’s parameters and set a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = 2200 * 8\n",
    "hidden_dim = 128\n",
    "output_dim = 6\n",
    "\n",
    "model = MultiLabelClassifier(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model ready, we now load the saved dataset and prepare it for iteration using PyTorch’s `DataLoader`, which handles batching and shuffling of the data.\n",
    "\n",
    "First, we load the entire dataset from the previously saved `dataset.pt` file using `torch.load`. This loads both the signal data and their associated labels.\n",
    "\n",
    "We then pass the loaded arrays to our previously defined `ECGDataset` class, which wraps the data in a format that PyTorch can use.\n",
    "\n",
    "Finally, we create a `DataLoader`, which enables efficient batch processing during training:\n",
    "- `batch_size=32`: Each training step will operate on 32 ECG samples.\n",
    "- `shuffle=True`: Randomly shuffles the dataset at the beginning of each epoch, helping the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_load = torch.load(\"dataset.pt\", weights_only=False)\n",
    "dataset = ECGDataset(dataset_load[\"data\"], dataset_load[\"labels\"])\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model, dataset, and training utilities ready, we now move into the training and evaluation phase. This is the most critical part of the notebook, where the model learns from the data and improves its predictions over time.\n",
    "\n",
    "We begin by setting the number of training epochs to **100**. Each epoch consists of two phases: training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Phase**\n",
    "\n",
    "At the start of each epoch, we switch the model to training mode using `model.train()`. This ensures that all layers behave appropriately during training (e.g., dropout remains active).\n",
    "\n",
    "We then initialize counters to track the total training loss and the number of correct predictions for calculating accuracy.\n",
    "\n",
    "Next, we iterate through each batch from the `data_loader`. For each batch:\n",
    "- We move the inputs and labels to the appropriate device (CPU or GPU).\n",
    "- We convert the labels to `float` type to match the expected input type for the binary cross-entropy loss.\n",
    "- We clear the optimizer gradients from the previous batch.\n",
    "- We pass the inputs through the model to obtain predictions.\n",
    "- We compute the loss using `BCELoss`, which is suitable for multi-label classification problems.\n",
    "- We backpropagate the loss using `.backward()` and update the model parameters with `.step()`.\n",
    "\n",
    "After updating the model, we calculate the training accuracy for the batch:\n",
    "- We threshold the outputs at 0.5 to produce binary predictions.\n",
    "- We count the number of correct predictions compared to the ground truth labels.\n",
    "- We accumulate the loss and correct predictions for calculating average metrics later.\n",
    "\n",
    "Once all batches are processed, we calculate:\n",
    "- **Average training loss** by dividing the total loss by the number of batches.\n",
    "- **Training accuracy** by dividing the total number of correct predictions by the total number of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Phase**\n",
    "\n",
    "After training, we switch the model to evaluation mode using `model.eval()`. This disables any training-specific behavior like dropout layers.\n",
    "\n",
    "We also wrap the evaluation logic inside a `torch.inference_mode()` context, which improves performance and reduces memory usage by disabling gradient computation.\n",
    "\n",
    "The evaluation loop is similar to the training loop, but without any gradient updates:\n",
    "- We iterate over the same `data_loader`.\n",
    "- For each batch, we perform a forward pass and compute the loss.\n",
    "- Predictions are thresholded at 0.5, and accuracy is calculated in the same way as during training.\n",
    "\n",
    "After all batches have been evaluated, we calculate:\n",
    "- **Average test loss**\n",
    "- **Test accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.3363, Train Acc: 0.9710 | Test Loss: 0.2585, Test Acc: 0.9767\n",
      "Epoch 2/100 | Train Loss: 0.2820, Train Acc: 0.9765 | Test Loss: 1.2255, Test Acc: 0.9612\n",
      "Epoch 3/100 | Train Loss: 0.2638, Train Acc: 0.9763 | Test Loss: 0.2426, Test Acc: 0.9778\n",
      "Epoch 4/100 | Train Loss: 0.2510, Train Acc: 0.9768 | Test Loss: 0.2279, Test Acc: 0.9780\n",
      "Epoch 5/100 | Train Loss: 0.2510, Train Acc: 0.9770 | Test Loss: 0.2321, Test Acc: 0.9788\n",
      "Epoch 6/100 | Train Loss: 0.2636, Train Acc: 0.9774 | Test Loss: 0.2591, Test Acc: 0.9791\n",
      "Epoch 7/100 | Train Loss: 0.2885, Train Acc: 0.9778 | Test Loss: 0.2713, Test Acc: 0.9797\n",
      "Epoch 8/100 | Train Loss: 0.2775, Train Acc: 0.9785 | Test Loss: 0.2980, Test Acc: 0.9809\n",
      "Epoch 9/100 | Train Loss: 0.2859, Train Acc: 0.9797 | Test Loss: 0.2692, Test Acc: 0.9821\n",
      "Epoch 10/100 | Train Loss: 0.2854, Train Acc: 0.9807 | Test Loss: 0.2712, Test Acc: 0.9833\n",
      "Epoch 11/100 | Train Loss: 0.2974, Train Acc: 0.9814 | Test Loss: 0.2908, Test Acc: 0.9838\n",
      "Epoch 12/100 | Train Loss: 0.3010, Train Acc: 0.9823 | Test Loss: 0.2825, Test Acc: 0.9855\n",
      "Epoch 13/100 | Train Loss: 0.3011, Train Acc: 0.9838 | Test Loss: 0.2772, Test Acc: 0.9871\n",
      "Epoch 14/100 | Train Loss: 0.3191, Train Acc: 0.9842 | Test Loss: 0.3114, Test Acc: 0.9862\n",
      "Epoch 15/100 | Train Loss: 0.3139, Train Acc: 0.9850 | Test Loss: 0.3290, Test Acc: 0.9867\n",
      "Epoch 16/100 | Train Loss: 0.3127, Train Acc: 0.9860 | Test Loss: 0.2859, Test Acc: 0.9888\n",
      "Epoch 17/100 | Train Loss: 0.3120, Train Acc: 0.9869 | Test Loss: 0.2993, Test Acc: 0.9886\n",
      "Epoch 18/100 | Train Loss: 0.3282, Train Acc: 0.9862 | Test Loss: 0.3364, Test Acc: 0.9848\n",
      "Epoch 19/100 | Train Loss: 0.3110, Train Acc: 0.9875 | Test Loss: 0.3009, Test Acc: 0.9891\n",
      "Epoch 20/100 | Train Loss: 0.3121, Train Acc: 0.9879 | Test Loss: 0.2980, Test Acc: 0.9896\n",
      "Epoch 21/100 | Train Loss: 0.3147, Train Acc: 0.9879 | Test Loss: 0.3102, Test Acc: 0.9891\n",
      "Epoch 22/100 | Train Loss: 0.3157, Train Acc: 0.9880 | Test Loss: 0.3149, Test Acc: 0.9881\n",
      "Epoch 23/100 | Train Loss: 0.3110, Train Acc: 0.9884 | Test Loss: 0.2943, Test Acc: 0.9900\n",
      "Epoch 24/100 | Train Loss: 0.3062, Train Acc: 0.9892 | Test Loss: 0.2954, Test Acc: 0.9902\n",
      "Epoch 25/100 | Train Loss: 0.3209, Train Acc: 0.9884 | Test Loss: 0.3133, Test Acc: 0.9896\n",
      "Epoch 26/100 | Train Loss: 0.3164, Train Acc: 0.9887 | Test Loss: 0.3038, Test Acc: 0.9899\n",
      "Epoch 27/100 | Train Loss: 0.3284, Train Acc: 0.9883 | Test Loss: 0.3057, Test Acc: 0.9896\n",
      "Epoch 28/100 | Train Loss: 0.3218, Train Acc: 0.9886 | Test Loss: 0.2985, Test Acc: 0.9904\n",
      "Epoch 29/100 | Train Loss: 0.3126, Train Acc: 0.9889 | Test Loss: 0.2984, Test Acc: 0.9902\n",
      "Epoch 30/100 | Train Loss: 0.3125, Train Acc: 0.9891 | Test Loss: 0.3005, Test Acc: 0.9904\n",
      "Epoch 31/100 | Train Loss: 0.3151, Train Acc: 0.9892 | Test Loss: 0.3090, Test Acc: 0.9900\n",
      "Epoch 32/100 | Train Loss: 0.3061, Train Acc: 0.9896 | Test Loss: 0.3000, Test Acc: 0.9903\n",
      "Epoch 33/100 | Train Loss: 0.3209, Train Acc: 0.9887 | Test Loss: 0.3132, Test Acc: 0.9897\n",
      "Epoch 34/100 | Train Loss: 0.3214, Train Acc: 0.9888 | Test Loss: 0.3196, Test Acc: 0.9896\n",
      "Epoch 35/100 | Train Loss: 0.3185, Train Acc: 0.9893 | Test Loss: 0.3144, Test Acc: 0.9901\n",
      "Epoch 36/100 | Train Loss: 0.3132, Train Acc: 0.9894 | Test Loss: 0.2987, Test Acc: 0.9905\n",
      "Epoch 37/100 | Train Loss: 0.3212, Train Acc: 0.9891 | Test Loss: 0.3030, Test Acc: 0.9885\n",
      "Epoch 38/100 | Train Loss: 0.3241, Train Acc: 0.9892 | Test Loss: 0.3081, Test Acc: 0.9904\n",
      "Epoch 39/100 | Train Loss: 0.3165, Train Acc: 0.9897 | Test Loss: 0.3012, Test Acc: 0.9907\n",
      "Epoch 40/100 | Train Loss: 0.3084, Train Acc: 0.9898 | Test Loss: 0.3382, Test Acc: 0.9893\n",
      "Epoch 41/100 | Train Loss: 0.3112, Train Acc: 0.9895 | Test Loss: 0.3160, Test Acc: 0.9891\n",
      "Epoch 42/100 | Train Loss: 0.3208, Train Acc: 0.9894 | Test Loss: 0.3105, Test Acc: 0.9900\n",
      "Epoch 43/100 | Train Loss: 0.3117, Train Acc: 0.9894 | Test Loss: 0.3617, Test Acc: 0.9856\n",
      "Epoch 44/100 | Train Loss: 0.3174, Train Acc: 0.9894 | Test Loss: 0.2985, Test Acc: 0.9909\n",
      "Epoch 45/100 | Train Loss: 0.3056, Train Acc: 0.9900 | Test Loss: 0.3011, Test Acc: 0.9907\n",
      "Epoch 46/100 | Train Loss: 0.3202, Train Acc: 0.9894 | Test Loss: 0.3152, Test Acc: 0.9901\n",
      "Epoch 47/100 | Train Loss: 0.3153, Train Acc: 0.9898 | Test Loss: 0.3040, Test Acc: 0.9908\n",
      "Epoch 48/100 | Train Loss: 0.3068, Train Acc: 0.9899 | Test Loss: 0.2962, Test Acc: 0.9908\n",
      "Epoch 49/100 | Train Loss: 0.3153, Train Acc: 0.9898 | Test Loss: 0.3077, Test Acc: 0.9905\n",
      "Epoch 50/100 | Train Loss: 0.3199, Train Acc: 0.9893 | Test Loss: 0.3048, Test Acc: 0.9908\n",
      "Epoch 51/100 | Train Loss: 0.3259, Train Acc: 0.9895 | Test Loss: 0.3082, Test Acc: 0.9903\n",
      "Epoch 52/100 | Train Loss: 0.3085, Train Acc: 0.9898 | Test Loss: 0.3193, Test Acc: 0.9901\n",
      "Epoch 53/100 | Train Loss: 0.3081, Train Acc: 0.9901 | Test Loss: 0.3029, Test Acc: 0.9908\n",
      "Epoch 54/100 | Train Loss: 0.3051, Train Acc: 0.9899 | Test Loss: 0.3004, Test Acc: 0.9909\n",
      "Epoch 55/100 | Train Loss: 0.3126, Train Acc: 0.9897 | Test Loss: 0.3119, Test Acc: 0.9903\n",
      "Epoch 56/100 | Train Loss: 0.3141, Train Acc: 0.9895 | Test Loss: 0.3076, Test Acc: 0.9906\n",
      "Epoch 57/100 | Train Loss: 0.3139, Train Acc: 0.9900 | Test Loss: 0.3134, Test Acc: 0.9901\n",
      "Epoch 58/100 | Train Loss: 0.3164, Train Acc: 0.9899 | Test Loss: 0.3002, Test Acc: 0.9911\n",
      "Epoch 59/100 | Train Loss: 0.3046, Train Acc: 0.9905 | Test Loss: 0.2986, Test Acc: 0.9909\n",
      "Epoch 60/100 | Train Loss: 0.3308, Train Acc: 0.9893 | Test Loss: 0.3117, Test Acc: 0.9907\n",
      "Epoch 61/100 | Train Loss: 0.3116, Train Acc: 0.9900 | Test Loss: 0.3089, Test Acc: 0.9902\n",
      "Epoch 62/100 | Train Loss: 0.3044, Train Acc: 0.9904 | Test Loss: 0.2976, Test Acc: 0.9912\n",
      "Epoch 63/100 | Train Loss: 0.3105, Train Acc: 0.9902 | Test Loss: 0.3096, Test Acc: 0.9903\n",
      "Epoch 64/100 | Train Loss: 0.3044, Train Acc: 0.9901 | Test Loss: 0.3256, Test Acc: 0.9899\n",
      "Epoch 65/100 | Train Loss: 0.3256, Train Acc: 0.9895 | Test Loss: 0.3169, Test Acc: 0.9902\n",
      "Epoch 66/100 | Train Loss: 0.3154, Train Acc: 0.9899 | Test Loss: 0.3158, Test Acc: 0.9905\n",
      "Epoch 67/100 | Train Loss: 0.3102, Train Acc: 0.9902 | Test Loss: 0.3128, Test Acc: 0.9902\n",
      "Epoch 68/100 | Train Loss: 0.3171, Train Acc: 0.9898 | Test Loss: 0.3071, Test Acc: 0.9909\n",
      "Epoch 69/100 | Train Loss: 0.3163, Train Acc: 0.9902 | Test Loss: 0.3062, Test Acc: 0.9908\n",
      "Epoch 70/100 | Train Loss: 0.3184, Train Acc: 0.9898 | Test Loss: 0.2975, Test Acc: 0.9911\n",
      "Epoch 71/100 | Train Loss: 0.3279, Train Acc: 0.9896 | Test Loss: 0.3064, Test Acc: 0.9908\n",
      "Epoch 72/100 | Train Loss: 0.3139, Train Acc: 0.9902 | Test Loss: 0.3044, Test Acc: 0.9911\n",
      "Epoch 73/100 | Train Loss: 0.3063, Train Acc: 0.9905 | Test Loss: 0.3085, Test Acc: 0.9904\n",
      "Epoch 74/100 | Train Loss: 0.3128, Train Acc: 0.9901 | Test Loss: 0.3001, Test Acc: 0.9911\n",
      "Epoch 75/100 | Train Loss: 0.3060, Train Acc: 0.9902 | Test Loss: 0.2956, Test Acc: 0.9914\n",
      "Epoch 76/100 | Train Loss: 0.3006, Train Acc: 0.9904 | Test Loss: 0.2995, Test Acc: 0.9910\n",
      "Epoch 77/100 | Train Loss: 0.3137, Train Acc: 0.9900 | Test Loss: 0.3040, Test Acc: 0.9906\n",
      "Epoch 78/100 | Train Loss: 0.3037, Train Acc: 0.9905 | Test Loss: 0.2973, Test Acc: 0.9913\n",
      "Epoch 79/100 | Train Loss: 0.3018, Train Acc: 0.9903 | Test Loss: 0.3014, Test Acc: 0.9911\n",
      "Epoch 80/100 | Train Loss: 0.3182, Train Acc: 0.9901 | Test Loss: 0.3342, Test Acc: 0.9901\n",
      "Epoch 81/100 | Train Loss: 0.3100, Train Acc: 0.9903 | Test Loss: 0.2957, Test Acc: 0.9914\n",
      "Epoch 82/100 | Train Loss: 0.3008, Train Acc: 0.9905 | Test Loss: 0.2929, Test Acc: 0.9913\n",
      "Epoch 83/100 | Train Loss: 0.2992, Train Acc: 0.9908 | Test Loss: 0.2930, Test Acc: 0.9914\n",
      "Epoch 84/100 | Train Loss: 0.3169, Train Acc: 0.9900 | Test Loss: 0.3122, Test Acc: 0.9907\n",
      "Epoch 85/100 | Train Loss: 0.3190, Train Acc: 0.9899 | Test Loss: 0.3013, Test Acc: 0.9909\n",
      "Epoch 86/100 | Train Loss: 0.3039, Train Acc: 0.9906 | Test Loss: 0.3093, Test Acc: 0.9911\n",
      "Epoch 87/100 | Train Loss: 0.3081, Train Acc: 0.9905 | Test Loss: 0.2973, Test Acc: 0.9914\n",
      "Epoch 88/100 | Train Loss: 0.3013, Train Acc: 0.9907 | Test Loss: 0.2979, Test Acc: 0.9914\n",
      "Epoch 89/100 | Train Loss: 0.3144, Train Acc: 0.9904 | Test Loss: 0.3008, Test Acc: 0.9910\n",
      "Epoch 90/100 | Train Loss: 0.3154, Train Acc: 0.9901 | Test Loss: 0.3095, Test Acc: 0.9906\n",
      "Epoch 91/100 | Train Loss: 0.3154, Train Acc: 0.9902 | Test Loss: 0.2980, Test Acc: 0.9914\n",
      "Epoch 92/100 | Train Loss: 0.3028, Train Acc: 0.9908 | Test Loss: 0.2970, Test Acc: 0.9914\n",
      "Epoch 93/100 | Train Loss: 0.3077, Train Acc: 0.9906 | Test Loss: 0.3159, Test Acc: 0.9908\n",
      "Epoch 94/100 | Train Loss: 0.3078, Train Acc: 0.9904 | Test Loss: 0.3013, Test Acc: 0.9913\n",
      "Epoch 95/100 | Train Loss: 0.3136, Train Acc: 0.9901 | Test Loss: 0.3054, Test Acc: 0.9911\n",
      "Epoch 96/100 | Train Loss: 0.3048, Train Acc: 0.9903 | Test Loss: 0.2990, Test Acc: 0.9911\n",
      "Epoch 97/100 | Train Loss: 0.3014, Train Acc: 0.9905 | Test Loss: 0.3285, Test Acc: 0.9901\n",
      "Epoch 98/100 | Train Loss: 0.3068, Train Acc: 0.9907 | Test Loss: 0.2956, Test Acc: 0.9914\n",
      "Epoch 99/100 | Train Loss: 0.3024, Train Acc: 0.9909 | Test Loss: 0.2971, Test Acc: 0.9909\n",
      "Epoch 100/100 | Train Loss: 0.3115, Train Acc: 0.9902 | Test Loss: 0.3067, Test Acc: 0.9904\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.numel()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(data_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.float()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.numel()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(data_loader)\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} | Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"ecg_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Prediction Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained and evaluated across several epochs, we may want to inspect how it performs on individual samples. This can help us understand the model’s confidence and how it interprets specific ECG signal inputs.\n",
    "\n",
    "We define a helper function `print_pred()` to test the model’s predictions for a single sample. This function takes two arguments:\n",
    "- `vals`: The input ECG signal vector (one sample).\n",
    "- `ans`: The ground truth label(s) for that sample, used for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred(vals, ans):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        vals = vals.clone().detach().float().to(device)\n",
    "\n",
    "        vals = vals.view(1, -1)\n",
    "\n",
    "        pred = model(vals)\n",
    "        probabilities = pred.cpu().numpy()\n",
    "        predicted_labels = (pred > 0.5).int().cpu().numpy()\n",
    "\n",
    "        print(f\"\\nProbabilities: {probabilities}, \\nPrediction: {predicted_labels}, \\nAnswer: {ans} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilities: [[6.8153909e-08 2.4420189e-07 5.6282011e-17 1.3305446e-13 8.2911463e-08\n",
      "  1.9636718e-07]], \n",
      "Prediction: [[0 0 0 0 0 0]], \n",
      "Answer: [0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "\n",
      "Probabilities: [[3.2594273e-25 5.2787104e-21 6.1008863e-14 1.0344354e-22 1.0000000e+00\n",
      "  9.3258457e-12]], \n",
      "Prediction: [[0 0 0 0 1 0]], \n",
      "Answer: [0. 0. 0. 0. 1. 0.] \n",
      "\n",
      "\n",
      "Probabilities: [[0. 0. 0. 0. 0. 0.]], \n",
      "Prediction: [[0 0 0 0 0 0]], \n",
      "Answer: [1. 0. 0. 0. 0. 0.] \n",
      "\n",
      "\n",
      "Probabilities: [[0.0000000e+00 3.2699278e-24 1.0000000e+00 1.2412947e-32 0.0000000e+00\n",
      "  1.1415087e-38]], \n",
      "Prediction: [[0 0 1 0 0 0]], \n",
      "Answer: [0. 0. 1. 0. 0. 0.] \n",
      "\n",
      "\n",
      "Probabilities: [[0. 0. 0. 0. 0. 0.]], \n",
      "Prediction: [[0 0 0 0 0 0]], \n",
      "Answer: [0. 0. 0. 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model = MultiLabelClassifier(input_dim, hidden_dim, output_dim).to(device)\n",
    "test_model.load_state_dict(torch.load(\"ecg_model.pth\", weights_only=False, map_location=device))\n",
    "\n",
    "sample_0 = dataset.data[0]\n",
    "sample_1 = dataset.data[1]\n",
    "sample_2 = dataset.data[2]\n",
    "sample_3 = dataset.data[3]\n",
    "sample_4 = dataset.data[4]\n",
    "\n",
    "label_0 = dataset.labels[0].cpu().numpy()\n",
    "label_1 = dataset.labels[1].cpu().numpy()\n",
    "label_2 = dataset.labels[2].cpu().numpy()\n",
    "label_3 = dataset.labels[3].cpu().numpy()\n",
    "label_4 = dataset.labels[4].cpu().numpy()\n",
    "\n",
    "\n",
    "print_pred(sample_0, label_0)\n",
    "print_pred(sample_1, label_1)\n",
    "print_pred(sample_2, label_2)\n",
    "print_pred(sample_3, label_3)\n",
    "print_pred(sample_4, label_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table presents the model's predictions on five test samples. For each sample, we compare the predicted labels to the true labels and assess the model's accuracy:\n",
    "\n",
    "| Sample | Predicted Labels         | True Labels             | Notes        |\n",
    "|--------|--------------------------|-------------------------|--------------|\n",
    "| #0     | [0, 0, 0, 0, 0, 0]       | [0, 0, 0, 0, 0, 0]      | ✅ Correct   |\n",
    "| #1     | [0, 0, 0, 0, 1, 0]       | [0, 0, 0, 0, 1, 0]      | ✅ Correct   |\n",
    "| #2     | [0, 0, 0, 0, 0, 0]       | [1, 0, 0, 0, 0, 0]      | ❌ Missed    |\n",
    "| #3     | [0, 0, 1, 0, 0, 0]       | [0, 0, 1, 0, 0, 0]      | ✅ Correct   |\n",
    "| #4     | [0, 0, 0, 0, 0, 0]       | [0, 0, 0, 0, 0, 0]      | ✅ Correct   |\n",
    "\n",
    "**Analysis of Results**\n",
    "\n",
    "The model demonstrated **accurate predictions** for most of the test samples, particularly for cases where all labels are zero or where there is a clear positive label.\n",
    "\n",
    "**Sample #2** exhibited a **missed label** (false negative), where the model incorrectly predicted all labels as zero despite the true label indicating a positive class for the first position. This could indicate that the model struggles with subtle or rare signal features.\n",
    "\n",
    "The **overall performance** suggests that the model is capable of correctly identifying the most prominent features in the ECG signals but may require further refinement or additional training data to improve its ability to handle more complex or nuanced cases.\n",
    "\n",
    "**Conclusion and Future Work**\n",
    "\n",
    "The multi-label classifier has shown promising results in classifying ECG signals, demonstrating good performance in many scenarios. However, there is room for improvement, especially in handling edge cases and minimizing false negatives.\n",
    "\n",
    "Next steps include:\n",
    "- **Hyperparameter tuning** to optimize the model for better generalization.\n",
    "- **Dataset expansion** or augmentation to expose the model to a more diverse set of ECG patterns.\n",
    "- **Model refinement** using more complex architectures, such as convolutional neural networks (CNNs), which are particularly effective for time-series data like ECG signals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
